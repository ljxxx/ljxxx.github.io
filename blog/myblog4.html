<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <!--[if IE]>
    <meta http-equiv="content-language" content="IE=edge,chrome=1">
    <![endif]-->
    <title>刘佳兴的个人网站</title>
    <!-- BOOTSTRAP CORE STYLE -->
    <link href="/assets/css/bootstrap.css" rel="stylesheet" />
    <!-- FONT AWESOME ICON STYLE -->
    <link href="/assets/css/font-awesome.css" rel="stylesheet" />
    <!-- CUSTOM STYLE CSS -->
    <link href="/assets/css/style.css" rel="stylesheet" />
    <link href="http://cdn.bootcss.com/highlight.js/8.0/styles/monokai_sublime.min.css" rel="stylesheet">
    <script src="http://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
    <script >hljs.initHighlightingOnLoad();</script>
    <style type="text/css">
        p{
            text-indent: 2em; /*em是相对单位，2em即现在一个字大小的两倍*/
        }
    </style>

</head>
<body>
<div id="header">
    <div class="overlay">
        <div class="container">
            <div class="row">
                <div class="col-md-4 logo-div">
                    <div class="logo-inner text-center">
                        <div class="logo-name">
                            <a href="/index.html">
                                <img src="/assets/img/me.jpg" class="img-circle" />
                            </a>
                        </div>

                    </div>

                </div>
                <div class="col-md-8 header-text-top " id="about">



                    <h1>刘佳兴的个人博客</h1>
                    记录我的学习历程。。。<br/>
                    <h3>深度学习之caffe的使用</h3>
                    <h3>点击头像返回首页</h3>
                </div>
            </div>
        </div>
    </div>
</div>
<!--END HEADER SECTION-->
<div class="info-sec">
    <div class="container">
        <div class="row">
            <div class="col-md-10">
                <a href="/index.html">回到首页</a>
                <strong>联系我</strong>jiaixnglaw@163.com
            </div>
            <div class="col-md-2">
                <div class="social-link">

                </div>

            </div>

        </div>
    </div>
</div>
<!--END INFO SECTION-->
<div class="container">
    < class="row">
        < class="col-md-8 ">
            < class="blog-post">
                <!-- 放内容 -->
        <h3>了解caffe</h3>
        <p>
            参考资料:<a href="https://software.intel.com/zh-cn/articles/training-and-deploying-deep-learning-networks-with-caffe-optimized-for-intel-architecture#Examples">官方教程</a>
            caffe的<br/>
            Caffe* 是伯克利愿景和学习中心 (BVLC) 开发的深度学习框架。 该框架使用 C++ 和 CUDA* C++ 语言编写，并采用 Python* 和 MATLAB* 封装程序，
            非常适用于卷积神经网络、递归神经网络和多层感知器。 主 Caffe 分支 有多个不同的分解，支持 检测和分类、分割以及兼容的Spark等。<br/>
        <ul>
            <li>卷积神经网络（Convolutional Neural Network,CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。
                CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。
            </li>
            <li>递归神经网络（RNN）是两种人工神经网络的总称。一种是时间递归神经网络（recurrent neural network），另一种是结构递归神经网络（recursive neural network）。<br/>
                时间递归神经网络的神经元间连接构成有向图，而结构递归神经网络利用相似的神经网络结构递归构造更为复杂的深度网络。两者训练的算法不同，但属于同一算法变体。<br/>

                神经网络可划分成不同的种类。按连接方式来分主要有两种：前向神经网络和反馈(递归)神经网络。前向网络主要是函数映射，可用于模式识别和函数逼近。递归神经网络因为有反馈的存在，
                所以它是一个非线性动力系统，可用来实现联想记忆和求解优化等问题。
            </li>
            <li>
                多层感知器（MLP，Multilayer Perceptron）是一种前馈人工神经网络模型，其将输入的多个数据集映射到单一的输出的数据集上。用多个感知器实现非线性--多层感知神经网络。
            </li>
        </ul>
        <h3>caffe的模型剖析</h3>
        <a href="http://blog.sina.com.cn/s/blog_7317a3740102waup.html">参考资料</a> <br/>
        Caffe使用blob存储、交换、操纵这些信息。blob是整个框架的标准的数组结构和统一存储接口。
        layer作为建模和计算的基础，net作为layer的集合和链接。blob的细节描述了信息是怎样在layers和nets间存储和交换的.<br/>
        <h4>Blob--存储和通信</h4>
        Blob 是Caffe处理和传输的真实数据的包装类，同时它还隐含提供了在CPU和GPU之间同步数据的能力。在数学上，一个blob就是一个N维的数组，它是按照c语言风格存储的。<br/>

        caffe使用blob存储和交换数据。blob对不同数据提供了统一的内存接口；例如：一批图片，模型参数，优化过程的偏导数等。<br/>

        Blob通过在需要时将数据从CPU同步到GPU来隐藏在GPU/CPU之间进行混合操作的计算开销和精力开销.host 和device上的内存都是惰性分配的，从而能够高效使用存储空间。<br/>

        传统的图片数据维数为图片数量N x 通道数K x 高度H x 宽度W。由于Blob的内存布局是行优先,所以最右边／后边的维度变化的最快。
        例如，在一个4D的blob里，下标(n,k,h,w)在物理内存中位于下标((n*K+k)*H+h)*W+w).<br/>
        <ul>
            <li>
                Number / N 是数据的 batch size.批处理能获得更大的在GPU设备上的吞吐量。例如，对于ImageNet数据训练一批２５６个图片,N=256.
            </li>
            <li>
                通道数 / K 是 feature dimension, 例如RGB图片就是３通道的　K = 3.灰度　K=1
            </li>
        </ul>
        注意，尽管Caffe样例中的大多数blob都是4D的带坐标的图片应用，在非图片应用使用Blob也是完全可以的。例如，你仅仅需要一个全连接网络（比如传统多层感知机），
        使用一个2D的blob(shape(N,D))，调用InnerProductLayer就可以了。<br/>

        参数blob的维度随着layer的配置和类型变化。例如，对一个有９６个filters，每个filter有１１X１１的空域维度和３个输入的卷积层,它的blob维数为: 96 x 3 x 11 x 11.
        对于一个有着1000个输出和1024个输入的内积 / 全连接layer,它的参数blob是1000 x 1024。<br/>

        由于我们经常对blob的值和梯度感兴趣，所以blob存储了２块data和diff.前者是正常的传输数据，后者是网络计算的梯度。
        <h4>层的连接和计算</h4>
        layer是一个模型的精华所在，它也是计算的基本单元。layer包括了filter过滤,pool池化,进行inner product计算，
        应用诸如rectified-linear和sigmoid等元素级的非线性变换,正则化，读取数据,计算诸如softmax或hinge等代价损失。<br/>

        每个layer都定义了３个关键的计算操作：setup,forward和backward.<br/>

        <ul>
            <li>
                Setup: 在模型初始化的时候初始化layer和它的connections
            </li>
            <li>
                Forward: 从底层给出输入并计算输出，然后发送给顶层
            </li>
            <li>
                Backward: 给出顶层输出的梯度，计算输入的梯度，然后发送给底层。一个有参数的层会计算关于参数的梯度然后在内部存储这些梯度。
            </li>
        </ul>
        更详细的说，caffe将会实现２个Forward和Backward函数，一个给CPU，另一个给GPU.如果你没有实现GPU版本，那么layer就会退化成CPU函数作为一个后备选项。
        如果你要做快速实验，这个会用起来很顺手，尽管它会造成附加的数据传输开销（它的输入会从GPU复制到CPU,并且它的输出会从CPU拷贝到GPU);<br/>

        layer在把网络当做整体进行操作的时候有两个关键责任：前向传播从输入计算输出，反向传播获取输出的梯度，然后根据参数向前计算输入的梯度，
        这些梯度再依次向前传播。这些过程都是简单的前向传播和后向传播的组合。<br/>

        开发自己定义的层需要一点很小的努力，需要学习网络的组织和代码的模块画。定义每层的setup,forward,backward，然后你定义的这个层就可以包含进一个网络了。
        <h4>网络定义和操作</h4>
        net通过组合和自动求导联合定义了一个函数和它的导数.把每一层output组合起来计算一个特定任务的函数，把每一层的backward组合起来从loss计算梯度来学习该任务。
        Caffe模型是端对端的机器学习引擎。（这个和Theano类似的）<br/>

        net可以看做一个由layers组成的计算图(computation graph)，确切的说是一个有向无环图。Caffe为所有层做了所有bookkeeping的工作来保证前向传播和后向传播的正确性。
        典型的net由一个从磁盘读取数据的数据层开始，以一个loss层结束，是计算诸如分类或者重构之类的目标任务的。<br/>
        </p>
        <p>
        <h3>多层感知器介绍</h3><a href="http://blog.chinaunix.net/uid-26275986-id-4985394.html">参考资料</a>
        感知器（Perception）是基本的处理元素，它具有输入、输出，每个输入关联一个连接权重（connection weight），然后输出是输入的加权和 <br/>
        <img class="img-loc" src="/assets/img/blog401.jpeg">
        上图就是一个单层的感知器，输入分别是X0、X1、X2，输出Y是输入的加权和： <br/>
        Y = W0X0 + W1X1 + W2X2 <br/>
        在实际的使用中，我们的主要任务就是通过数据训练确定参数权重。在训练神经网络时，如果未提供全部样本二室逐个提供实例，则我们通常使用在线学习，
        然后在每个实例学习之后立刻调整网络参数，以这种方式使得网络缓慢得及时调整。具体收敛可是使用梯度下降算法。 <br/>

        感知器具有很强的表现力，比如布尔函数AND和OR都可以使用上面的单层感知器实现。但是对于XOR操作则不行，因为单层感知器只能模拟线性函数，对于XOR这种非线性函数，我们需要新型的感知器。 <br/>

        多层感知器（Multiayer perceptrons, MLP）可以实现非线性判别式，如果用于回归，可以逼近输入的非线性函数。其实MLP可以用于“普适近似”，即可以证明：
        具有连续输入和输出的任何函数都可以用MLP近似 ，已经证明，具有一个隐藏层（隐藏节点个数不限）的MLP可以学习输入的任意非线性函数。<br/>
        <img class="img-loc" src="/assets/img/blog402.jpeg">
        训练MLP常用的是向后传播（backpropagation），这主要是因为在我们收敛误差函数的时候，使用链接规则计算梯度：<br/>
        <img class="img-loc" src="/assets/img/blog403.jpeg">
        </p>
        <p>
            <h3>caffe的使用之入门</h3>
    <a href="http://christopher5106.github.io/deep/learning/2015/09/04/Deep-learning-tutorial-on-Caffe-Technology.html">参考资料</a>,原文为英文文献。
    <ul>
        <li>
            在Ubuntu中打开控制台,进入Python,先import相关内容,再设置运行模式为使用CPU-ONLY,如下图所示
            <img class="img-loc" src="/assets/img/blog404.jpeg">
        </li>
        <li>
            下一步,定义网络模型。网络模型通常在 conv.prototxt 中定义,一个conv.prototxt的示例如下:
            <img class="img-loc" src="/assets/img/blog405.jpeg">
            加载网络;
            此外,还需要定义好输入和输出,如:
            <pre>
                <code>
                    output = (input - kernel_size) / stride + 1;
                    net = caffe.Net('conv.prototxt', caffe.TEST);
                    print net.inputs;
                </code>
                而net中包括的内容主要有两部分:
                net.blobs和net.params
                <code>
                    [(k, v.data.shape) for k, v in net.blobs.items()]
                    [(k, v[0].data.shape, v[1].data.shape) for k, v in net.params.items()]

                </code>
            </pre>


            
        </li>
        <li></li>
        <li></li>
        <li></li>
    </ul>

        </p>
            </div>
        </div>
        <div class="col-md-1"></div>
        <div class="col-md-3" style="padding-top: 30px;">
            <div class="row">
                <ul class="list-group">
                    <li class="list-group-item"><strong>分类</strong></li>
                    <li class="list-group-item">Java基础</li>
                    <li class="list-group-item">JAVA WEB</li>
                    <li class="list-group-item">数据库</li>
                    <li class="list-group-item">数据结构</li>
                    <li class="list-group-item">计算机网络</li>
                    <li class="list-group-item">面经</li>
                    <li class="list-group-item">设计模式</li>
                    <li class="list-group-item">杂</li>

                </ul>
            </div>
            <div class="row">
                <h3>Advertising</h3>


            </div>
        </div>

    </div>


</div>

<!--END HOME PAGE SECTION-->
<div class="footer-sec" style="margin-top: 0px;">
    <div class="container">
        <div class="row">
            <div class="col-md-12 foo-inner">
                &copy;  ljxxx.github.io
            </div>
        </div>
    </div>
</div>
<!-- END FOOTER SECTION -->
<!-- JAVASCRIPT FILES PLACED AT THE BOTTOM TO REDUCE THE LOADING TIME -->
<!-- CORE JQUERY -->
<script src="/assets/js/jquery-1.11.1.js"></script>
<!-- BOOTSTRAP SCRIPTS -->
<script src="/assets/js/bootstrap.js"></script>

</body>
</html>
